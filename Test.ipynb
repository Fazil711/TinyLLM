{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 57\n"
     ]
    }
   ],
   "source": [
    "with open('Dataset.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = list(set(text))\n",
    "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256  # Context length\n",
    "data = torch.tensor([char_to_idx[c] for c in text], dtype=torch.long)\n",
    "n = int(0.9*len(data))  # 90% train, 10% val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        embed = self.embedding(x)  # Shape: (seq_len, batch_size, embedding_dim)\n",
    "        out, h = self.rnn(embed, h)\n",
    "        out = self.fc(out.view(-1, out.size(2)))  # Flatten for dense layer\n",
    "        return out, h\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.rnn.num_layers, batch_size, self.rnn.hidden_size).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=16,\n",
    "    hidden_dim=128,\n",
    "    n_layers=2\n",
    ")\n",
    "model = model.to('cuda')  # Use GPU\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 0, Loss: 4.0280\n",
      "Epoch 1/10, Step 10, Loss: 3.3424\n",
      "Epoch 1/10, Step 20, Loss: 3.0729\n",
      "Epoch 1/10, Step 30, Loss: 3.0374\n",
      "Epoch 1/10, Step 40, Loss: 3.0384\n",
      "Epoch 1/10, Step 50, Loss: 3.0433\n",
      "Epoch 1/10, Step 60, Loss: 3.0303\n",
      "Epoch 1/10, Step 70, Loss: 3.0305\n",
      "Epoch 1/10, Step 80, Loss: 3.0265\n",
      "Epoch 1/10, Step 90, Loss: 3.0338\n",
      "Epoch 2/10, Step 0, Loss: 3.0442\n",
      "Epoch 2/10, Step 10, Loss: 3.0412\n",
      "Epoch 2/10, Step 20, Loss: 3.0396\n",
      "Epoch 2/10, Step 30, Loss: 3.0200\n",
      "Epoch 2/10, Step 40, Loss: 3.0410\n",
      "Epoch 2/10, Step 50, Loss: 3.0388\n",
      "Epoch 2/10, Step 60, Loss: 3.0359\n",
      "Epoch 2/10, Step 70, Loss: 3.0292\n",
      "Epoch 2/10, Step 80, Loss: 3.0424\n",
      "Epoch 2/10, Step 90, Loss: 3.0368\n",
      "Epoch 3/10, Step 0, Loss: 3.0429\n",
      "Epoch 3/10, Step 10, Loss: 3.0224\n",
      "Epoch 3/10, Step 20, Loss: 3.0351\n",
      "Epoch 3/10, Step 30, Loss: 3.0334\n",
      "Epoch 3/10, Step 40, Loss: 3.0509\n",
      "Epoch 3/10, Step 50, Loss: 3.0376\n",
      "Epoch 3/10, Step 60, Loss: 3.0384\n",
      "Epoch 3/10, Step 70, Loss: 3.0426\n",
      "Epoch 3/10, Step 80, Loss: 3.0391\n",
      "Epoch 3/10, Step 90, Loss: 3.0378\n",
      "Epoch 4/10, Step 0, Loss: 3.0466\n",
      "Epoch 4/10, Step 10, Loss: 3.0262\n",
      "Epoch 4/10, Step 20, Loss: 3.0251\n",
      "Epoch 4/10, Step 30, Loss: 3.0361\n",
      "Epoch 4/10, Step 40, Loss: 3.0248\n",
      "Epoch 4/10, Step 50, Loss: 3.0423\n",
      "Epoch 4/10, Step 60, Loss: 3.0327\n",
      "Epoch 4/10, Step 70, Loss: 3.0374\n",
      "Epoch 4/10, Step 80, Loss: 3.0439\n",
      "Epoch 4/10, Step 90, Loss: 3.0292\n",
      "Epoch 5/10, Step 0, Loss: 3.0318\n",
      "Epoch 5/10, Step 10, Loss: 3.0362\n",
      "Epoch 5/10, Step 20, Loss: 3.0387\n",
      "Epoch 5/10, Step 30, Loss: 3.0414\n",
      "Epoch 5/10, Step 40, Loss: 3.0388\n",
      "Epoch 5/10, Step 50, Loss: 3.0497\n",
      "Epoch 5/10, Step 60, Loss: 3.0314\n",
      "Epoch 5/10, Step 70, Loss: 3.0432\n",
      "Epoch 5/10, Step 80, Loss: 3.0497\n",
      "Epoch 5/10, Step 90, Loss: 3.0579\n",
      "Epoch 6/10, Step 0, Loss: 3.0450\n",
      "Epoch 6/10, Step 10, Loss: 3.0347\n",
      "Epoch 6/10, Step 20, Loss: 3.0382\n",
      "Epoch 6/10, Step 30, Loss: 3.0337\n",
      "Epoch 6/10, Step 40, Loss: 3.0434\n",
      "Epoch 6/10, Step 50, Loss: 3.0544\n",
      "Epoch 6/10, Step 60, Loss: 3.0264\n",
      "Epoch 6/10, Step 70, Loss: 3.0318\n",
      "Epoch 6/10, Step 80, Loss: 3.0310\n",
      "Epoch 6/10, Step 90, Loss: 3.0332\n",
      "Epoch 7/10, Step 0, Loss: 3.0482\n",
      "Epoch 7/10, Step 10, Loss: 3.0335\n",
      "Epoch 7/10, Step 20, Loss: 3.0418\n",
      "Epoch 7/10, Step 30, Loss: 3.0442\n",
      "Epoch 7/10, Step 40, Loss: 3.0275\n",
      "Epoch 7/10, Step 50, Loss: 3.0392\n",
      "Epoch 7/10, Step 60, Loss: 3.0151\n",
      "Epoch 7/10, Step 70, Loss: 3.0336\n",
      "Epoch 7/10, Step 80, Loss: 3.0286\n",
      "Epoch 7/10, Step 90, Loss: 3.0382\n",
      "Epoch 8/10, Step 0, Loss: 3.0403\n",
      "Epoch 8/10, Step 10, Loss: 3.0429\n",
      "Epoch 8/10, Step 20, Loss: 3.0301\n",
      "Epoch 8/10, Step 30, Loss: 3.0352\n",
      "Epoch 8/10, Step 40, Loss: 3.0380\n",
      "Epoch 8/10, Step 50, Loss: 3.0351\n",
      "Epoch 8/10, Step 60, Loss: 3.0217\n",
      "Epoch 8/10, Step 70, Loss: 3.0268\n",
      "Epoch 8/10, Step 80, Loss: 3.0414\n",
      "Epoch 8/10, Step 90, Loss: 3.0408\n",
      "Epoch 9/10, Step 0, Loss: 3.0614\n",
      "Epoch 9/10, Step 10, Loss: 3.0348\n",
      "Epoch 9/10, Step 20, Loss: 3.0268\n",
      "Epoch 9/10, Step 30, Loss: 3.0329\n",
      "Epoch 9/10, Step 40, Loss: 3.0522\n",
      "Epoch 9/10, Step 50, Loss: 3.0335\n",
      "Epoch 9/10, Step 60, Loss: 3.0293\n",
      "Epoch 9/10, Step 70, Loss: 3.0371\n",
      "Epoch 9/10, Step 80, Loss: 3.0362\n",
      "Epoch 9/10, Step 90, Loss: 3.0423\n",
      "Epoch 10/10, Step 0, Loss: 3.0527\n",
      "Epoch 10/10, Step 10, Loss: 3.0311\n",
      "Epoch 10/10, Step 20, Loss: 3.0431\n",
      "Epoch 10/10, Step 30, Loss: 3.0444\n",
      "Epoch 10/10, Step 40, Loss: 3.0421\n",
      "Epoch 10/10, Step 50, Loss: 3.0283\n",
      "Epoch 10/10, Step 60, Loss: 3.0342\n",
      "Epoch 10/10, Step 70, Loss: 3.0394\n",
      "Epoch 10/10, Step 80, Loss: 3.0316\n",
      "Epoch 10/10, Step 90, Loss: 3.0483\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    hidden = model.init_hidden(batch_size).to('cuda')\n",
    "    for step in range(100):  # Train for 100 steps per epoch\n",
    "        inputs, targets = get_batch('train')\n",
    "        inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
    "\n",
    "        # Forward pass\n",
    "        hidden = hidden.detach()\n",
    "        logits, hidden = model(inputs.T, hidden)\n",
    "        loss = criterion(logits, targets.view(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(model.state_dict(), 'char_rnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a timeezo vgdh faednRinrerro dti oeaOe f slrhhestrhra edes eegketkihe teeRf rh rtsedo, tw til tursTalWTenusf    eopgtteimo gpsgsr . ee it h  ia ukag.thlu iico n ttsselegiA nhn albeeatirhteve\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def generate_text(model, start_str, max_length=500):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor([char_to_idx[c] for c in start_str], dtype=torch.long).view(-1, 1).to('cuda')\n",
    "        hidden = model.init_hidden(1).to('cuda')\n",
    "        generated = start_str\n",
    "\n",
    "        for _ in range(max_length - len(start_str)):\n",
    "            input_seq = x[-block_size:] if len(x) >= block_size else x\n",
    "            logits, hidden = model(input_seq, hidden)\n",
    "            prob = nn.functional.softmax(logits[-1], dim=0).cpu().numpy()\n",
    "            next_char_idx = np.random.choice(len(prob), p=prob)\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            generated += next_char\n",
    "            x = torch.cat((x, torch.tensor([[next_char_idx]]).to('cuda')), dim=0)\n",
    "\n",
    "    return generated\n",
    "\n",
    "print(generate_text(model, \"Once upon a time\", max_length=200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
